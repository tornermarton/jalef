{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is only for server (no colab func.)\n",
    "\n",
    "import os\n",
    "os.chdir(\"/app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "!find /app/data/ -name \"*_dataset_*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "!ls -l /app/logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constatnts\n",
    "MAX_SEQ_LEN = 128\n",
    "MIN_SEQ_LEN = int(MAX_SEQ_LEN / 2)\n",
    "\n",
    "PRETRAINED_MODEL_PATH = \"./models/bert/\"\n",
    "# modify if model changed to other H-XXXX!!!!\n",
    "OUTPUT_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "\n",
    "from IPython.display import display\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "from jalef.statistics import evaluate_result, compare_thresholds, select_errors, remove_uncertain_preds\n",
    "from jalef.plots import plot_confusion_matrix, enable_plotly_in_cell\n",
    "\n",
    "enable_plotly_in_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate single test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"coursera\"\n",
    "EMBEDDING = \"bert\"\n",
    "N_INTENTS = 100\n",
    "BALANCED = True\n",
    "CUSTOM_PARAMETERS = dict()\n",
    "CUSTOM_PARAMETERS_AS_STRING = \"_\".join([str(key) + \"=\" + str(value) for key, value in CUSTOM_PARAMETERS])\n",
    "\n",
    "# Please fill these\n",
    "DATASET_PATH = \"/app/data/{}_dataset_top{}\".format(SOURCE, N_INTENTS)\n",
    "TEST_TITLE = \"Predictions for the top {} intents in the {} dataset with {} training.\".format(N_INTENTS, SOURCE, \"balanced\" if BALANCED else \"normal\")\n",
    "TEST_NAME = \"dataset={}_embedding={}{}{}intents={}\".format(\n",
    "    SOURCE, \n",
    "    EMBEDDING, \n",
    "    \"_balanced_\" if BALANCED else \"_\", \n",
    "    CUSTOM_PARAMETERS_AS_STRING + \"_\" if CUSTOM_PARAMETERS_AS_STRING != \"\", \n",
    "    N_INTENTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and look-up tables\n",
    "dataset = pd.read_csv(os.path.join(DATASET_PATH, \"test.csv\"))\n",
    "lut = pd.read_csv(os.path.join(DATASET_PATH, \"lut.csv\"), names=[\"Intent\"], index_col=0, header=0)\n",
    "reverse_lut = pd.read_csv(os.path.join(DATASET_PATH, \"lut.csv\"), names=[\"Label\"], index_col=1, header=0)\n",
    "\n",
    "dataset[\"Intent\"] = [lut.at[label, \"Intent\"] for label in dataset[\"Label\"].values]\n",
    "\n",
    "print(\"The test set has {} rows.\".format(len(dataset)))\n",
    "display(dataset.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions for dataset\n",
    "preds = np.load(\n",
    "    os.path.join(os.path.join(\"/app/logs/\", TEST_NAME), predictions.npy\")\n",
    ")\n",
    "\n",
    "y_pred = np.array([lut.at[np.argmax(e), \"Intent\"] for e in preds])\n",
    "y_true = np.array([lut.at[e, \"Intent\"] for e in dataset[\"Label\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=False, title=TEST_TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the thresholds\n",
    "print(\"The performance of the model with different uncertainty thresholds:\")\n",
    "compare_thresholds(y_true, y_pred, preds, [0, 0.7, 0.8, 0.85, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These are the cases, where the classifier predicts the wrong class:\")\n",
    "errors = select_errors(y_true, y_pred, preds, reverse_lut)\n",
    "display(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show intent distribution in test set\n",
    "dataset.groupby(\"Intent\").count().sort_values(by=[\"Label\"], ascending=False)[\"Label\"].iplot(\n",
    "    kind=\"bar\",\n",
    "    xTitle=\"Intent\",\n",
    "    yTitle=\"Count\",\n",
    "    title=\"Intent distribution in dataset\",\n",
    "    tickfont=dict(\n",
    "        size=9,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare two tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_1 = \"coursera\"\n",
    "EMBEDDING_1 = \"bert\"\n",
    "N_INTENTS_1 = 100\n",
    "BALANCED_1 = True\n",
    "CUSTOM_PARAMETERS_1 = dict()\n",
    "CUSTOM_PARAMETERS_AS_STRING_1 = \"_\".join([str(key) + \"=\" + str(value) for key, value in CUSTOM_PARAMETERS_1])\n",
    "\n",
    "# Please fill these\n",
    "DATASET_PATH_1 = \"/app/data/{}_dataset_top{}\".format(SOURCE_1, N_INTENTS_1)\n",
    "TEST_TITLE_1 = \"Predictions for the top {} intents in the {} dataset with {} training.\".format(N_INTENTS_1, SOURCE_1, \"balanced\" if BALANCED_1 else \"normal\")\n",
    "TEST_NAME_1 = \"dataset={}_embedding={}{}{}intents={}\".format(\n",
    "    SOURCE_1, \n",
    "    EMBEDDING_1, \n",
    "    \"_balanced_\" if BALANCED_1 else \"_\", \n",
    "    CUSTOM_PARAMETERS_AS_STRING_1 + \"_\" if CUSTOM_PARAMETERS_AS_STRING_1 != \"\", \n",
    "    N_INTENTS_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_2 = \"coursera\"\n",
    "EMBEDDING_2 = \"bert\"\n",
    "N_INTENTS_2 = 100\n",
    "BALANCED_2 = True\n",
    "CUSTOM_PARAMETERS_2 = dict()\n",
    "CUSTOM_PARAMETERS_AS_STRING_2 = \"_\".join([str(key) + \"=\" + str(value) for key, value in CUSTOM_PARAMETERS_2])\n",
    "\n",
    "# Please fill these\n",
    "DATASET_PATH_2 = \"/app/data/{}_dataset_top{}\".format(SOURCE_2, N_INTENTS_2)\n",
    "TEST_TITLE_2 = \"Predictions for the top {} intents in the {} dataset with {} training.\".format(N_INTENTS_2, SOURCE_2, \"balanced\" if BALANCED_2 else \"normal\")\n",
    "TEST_NAME_2 = \"dataset={}_embedding={}{}{}intents={}\".format(\n",
    "    SOURCE_2, \n",
    "    EMBEDDING_2, \n",
    "    \"_balanced_\" if BALANCED_2 else \"_\", \n",
    "    CUSTOM_PARAMETERS_AS_STRING_2 + \"_\" if CUSTOM_PARAMETERS_AS_STRING_2 != \"\", \n",
    "    N_INTENTS_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset 1 and look-up tables\n",
    "dataset_1 = pd.read_csv(os.path.join(DATASET_PATH_1, \"test.csv\"))\n",
    "lut_1 = pd.read_csv(os.path.join(DATASET_PATH_1, \"lut.csv\"), names=[\"Intent\"], index_col=0, header=0)\n",
    "reverse_lut_1 = pd.read_csv(os.path.join(DATASET_PATH_1, \"lut.csv\"), names=[\"Label\"], index_col=1, header=0)\n",
    "\n",
    "dataset_1[\"Intent\"] = [lut_1.at[label, \"Intent\"] for label in dataset_1[\"Label\"].values]\n",
    "\n",
    "# load predictions for dataset\n",
    "preds_1 = np.load(\n",
    "    os.path.join(os.path.join(\"/app/logs/\", TEST_NAME_1), predictions.npy\")\n",
    ")\n",
    "\n",
    "y_pred_1 = np.array([lut_1.at[np.argmax(e), \"Intent\"] for e in preds_1])\n",
    "y_true_1 = np.array([lut_1.at[e, \"Intent\"] for e in dataset_1[\"Label\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset 2 and look-up tables\n",
    "dataset_2 = pd.read_csv(os.path.join(DATASET_PATH_2, \"test.csv\"))\n",
    "lut_2 = pd.read_csv(os.path.join(DATASET_PATH_2, \"lut.csv\"), names=[\"Intent\"], index_col=0, header=0)\n",
    "reverse_lut_2 = pd.read_csv(os.path.join(DATASET_PATH_2, \"lut.csv\"), names=[\"Label\"], index_col=1, header=0)\n",
    "\n",
    "dataset_2[\"Intent\"] = [lut_2.at[label, \"Intent\"] for label in dataset_2[\"Label\"].values]\n",
    "\n",
    "# load predictions for dataset\n",
    "preds_2 = np.load(\n",
    "    os.path.join(os.path.join(\"/app/logs/\", TEST_NAME_2), predictions.npy\")\n",
    ")\n",
    "\n",
    "y_pred_2 = np.array([lut_2.at[np.argmax(e), \"Intent\"] for e in preds_2])\n",
    "y_true_2 = np.array([lut_2.at[e, \"Intent\"] for e in dataset_2[\"Label\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accurracy, precision, etc.\n",
    "df = pd.DataFrame().from_dict(evaluate_result(y_true=y_true_1, y_pred=y_pred_1), columns=[TEST_TITLE_1], orient=\"index\")\n",
    "df[TEST_TITLE_2] = evaluate_result(y_true=y_true_2, y_pred=y_pred_2).values()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the confusion matrices\n",
    "plot_confusion_matrix(y_true=y_true_1, y_pred=y_pred_1, normalize=False, title=TEST_TITLE_1)\n",
    "plot_confusion_matrix(y_true=y_true_2, y_pred=y_pred_2, normalize=False, title=TEST_TITLE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the threshold results\n",
    "\n",
    "print(TEST_NAME_1)\n",
    "compare_thresholds(y_true_1, y_pred_1, preds_1, [0, 0.7, 0.8, 0.85, 0.9, 0.95])\n",
    "\n",
    "print(TEST_NAME_2)\n",
    "compare_thresholds(y_true_2, y_pred_2, preds_2, [0, 0.7, 0.8, 0.85, 0.9, 0.95])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
